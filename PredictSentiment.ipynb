{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kode random\\Web Bu akhsin\\Streamlit\\stenv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 0.22.2.post1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\kode random\\Web Bu akhsin\\Streamlit\\stenv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 0.22.2.post1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\kode random\\Web Bu akhsin\\Streamlit\\stenv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 0.22.2.post1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\kode random\\Web Bu akhsin\\Streamlit\\stenv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MLPClassifier from version 0.22.2.post1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load models and feature extraction\n",
    "feature_bow = pickle.load(open(\"model/feature-bow.p\", 'rb'))\n",
    "model_nb = pickle.load(open('model/model-nb.p', 'rb'))\n",
    "model_nn = pickle.load(open('model/model-nn.p', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing_with_stemming(string):\n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http[s]?://\\S+', '', string)  # Remove links\n",
    "    # Remove mentions (words starting with @) and any characters that follow\n",
    "    string = re.sub(r'@\\w+', '', string)  # Remove mentions\n",
    "    # Remove non-alphanumeric characters (except space)\n",
    "    string = re.sub(r'[^a-zA-Z0-9\\s]', ' ', string)\n",
    "    # Remove 'rt'\n",
    "    string = re.sub(r'\\brt\\b', '', string)  # Use word boundary \\b to match 'rt' exactly\n",
    "    # Remove extra spaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()  # Replace multiple spaces with a single space\n",
    "    \n",
    "    # Remove specific suffixes: \"lah\", \"kah\", \"pun\"\n",
    "    string = re.sub(r'\\b(\\w+)(lah|kah|pun)\\b', r'\\1', string)\n",
    "\n",
    "    # Remove duplicate sentences\n",
    "    string = ' '.join(sorted(set(string.split()), key=string.split().index))\n",
    "\n",
    "    # Stemming with Sastrawi\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    string = stemmer.stem(string)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  \n",
      "@FIFAWorldCup Hai FIFA ....kami Indonesia dan kami adalah tim yang paling berkembang dan energik di Asia tanpa bergantung pada wasit atau permainan kasar tidak seperti tim dari timur tengah dan anda diam melihat hal tsb tidak melakukan apapun padahal sepak bola adl. Tentang fair play,fifaworldcup hai fifa kami indonesia dan kami ada tim yang paling kembang dan energik di asia tanpa gantung pada wasit atau main kasar tidak seperti tim dari timur tengah dan anda diam lihat hal tsb tidak laku apa padahal sepak bola adl tentang fair play\n",
      "\n",
      "Cleaned Text:  hai fifa kami indonesia dan ada tim yang paling kembang energik di asia tanpa gantung pada wasit atau main kasar tidak seperti dari timur tengah anda diam lihat hal tsb laku apa padahal sepak bola adl tentang fair play fifaworldcup kembang gantung main lihat laku\n",
      "Sentiment NB:  negative\n",
      "Sentiment NN:  negative\n"
     ]
    }
   ],
   "source": [
    "# Original text\n",
    "original_text = '''\n",
    "@FIFAWorldCup Hai FIFA ....kami Indonesia dan kami adalah tim yang paling berkembang dan energik di Asia tanpa bergantung pada wasit atau permainan kasar tidak seperti tim dari timur tengah dan anda diam melihat hal tsb tidak melakukan apapun padahal sepak bola adl. Tentang fair play,fifaworldcup hai fifa kami indonesia dan kami ada tim yang paling kembang dan energik di asia tanpa gantung pada wasit atau main kasar tidak seperti tim dari timur tengah dan anda diam lihat hal tsb tidak laku apa padahal sepak bola adl tentang fair play\n",
    "'''\n",
    "\n",
    "# Apply the cleansing and stemming\n",
    "cleaned_text = cleansing_with_stemming(original_text)\n",
    "\n",
    "# Feature extraction and predict for original text\n",
    "text_feature = feature_bow.transform([cleaned_text])\n",
    "\n",
    "print(\"Text: \", original_text)\n",
    "print(\"Cleaned Text: \", cleaned_text)  # Print cleaned text for verification\n",
    "print(\"Sentiment NB: \", model_nb.predict(text_feature)[0])\n",
    "print(\"Sentiment NN: \", model_nn.predict(text_feature)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   conversation_id_str                      created_at  favorite_count  \\\n",
      "0  1844695415763120259  Fri Oct 11 23:56:01 +0000 2024               1   \n",
      "1  1844694787213148515  Fri Oct 11 23:55:47 +0000 2024               0   \n",
      "2  1844531990332309949  Fri Oct 11 23:55:34 +0000 2024               0   \n",
      "3  1840157006612558197  Fri Oct 11 23:55:14 +0000 2024               0   \n",
      "4  1844453665631031328  Fri Oct 11 23:55:00 +0000 2024               0   \n",
      "\n",
      "                                           full_text               id_str  \\\n",
      "0  @FIFAWorldCup Hai FIFA ....kami Indonesia dan ...  1844889725804609840   \n",
      "1  @emhendraka Nek ra itungan ngko koyo wasit sok...  1844889666774081576   \n",
      "2  @SiaranBolaLive Tak kasi sambel matanya ini wa...  1844889610125705340   \n",
      "3  @fifamedia Hai FIFA ....kami Indonesia ....kam...  1844889526646472831   \n",
      "4  @yellowsea_sun @idextratime Kenapa bisa lolos ...  1844889468484059456   \n",
      "\n",
      "  image_url in_reply_to_screen_name lang              location  quote_count  \\\n",
      "0       NaN            FIFAWorldCup   in                   NaN            0   \n",
      "1       NaN              emhendraka   in             Indonesia            0   \n",
      "2       NaN          SiaranBolaLive   in                   NaN            0   \n",
      "3       NaN               fifamedia   in                   NaN            0   \n",
      "4       NaN           yellowsea_sun   in  Lelaki yg mencari_Mu            0   \n",
      "\n",
      "   reply_count  retweet_count  \\\n",
      "0            0              0   \n",
      "1            0              0   \n",
      "2            0              0   \n",
      "3            0              0   \n",
      "4            0              0   \n",
      "\n",
      "                                           tweet_url          user_id_str  \\\n",
      "0  https://x.com/Wisnu77048883/status/18448897258...  1097355180775858176   \n",
      "1  https://x.com/mfatahaaaw/status/18448896667740...           2150391482   \n",
      "2  https://x.com/rayyan_azw15051/status/184488961...  1755908744603697152   \n",
      "3  https://x.com/Wisnu77048883/status/18448895266...  1097355180775858176   \n",
      "4  https://x.com/kipyiksamgong/status/18448894684...   985730749209788416   \n",
      "\n",
      "          username  \n",
      "0    Wisnu77048883  \n",
      "1       mfatahaaaw  \n",
      "2  rayyan_azw15051  \n",
      "3    Wisnu77048883  \n",
      "4    kipyiksamgong  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/wasit.csv', sep=',')\n",
    "print(data.head())  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['conversation_id_str', 'created_at', 'favorite_count', 'id_str', 'image_url', \n",
    "                   'in_reply_to_screen_name', 'lang', 'location', 'quote_count', 'reply_count', \n",
    "                   'retweet_count', 'tweet_url', 'user_id_str', 'username']\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FIFAWorldCup Hai FIFA ....kami Indonesia dan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@emhendraka Nek ra itungan ngko koyo wasit sok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SiaranBolaLive Tak kasi sambel matanya ini wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@fifamedia Hai FIFA ....kami Indonesia ....kam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@yellowsea_sun @idextratime Kenapa bisa lolos ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  @FIFAWorldCup Hai FIFA ....kami Indonesia dan ...\n",
       "1  @emhendraka Nek ra itungan ngko koyo wasit sok...\n",
       "2  @SiaranBolaLive Tak kasi sambel matanya ini wa...\n",
       "3  @fifamedia Hai FIFA ....kami Indonesia ....kam...\n",
       "4  @yellowsea_sun @idextratime Kenapa bisa lolos ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the column to 'Tweet' for consistency\n",
    "data = data.rename(columns={'full_text': 'Tweet'})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FIFAWorldCup Hai FIFA ....kami Indonesia dan ...</td>\n",
       "      <td>hai fifa kami indonesia dan ada tim yang palin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@emhendraka Nek ra itungan ngko koyo wasit sok...</td>\n",
       "      <td>nek ra itungan ngko koyo wasit soko om kae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SiaranBolaLive Tak kasi sambel matanya ini wa...</td>\n",
       "      <td>tak kasi sambel mata ini wasit dajal biar seka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@fifamedia Hai FIFA ....kami Indonesia ....kam...</td>\n",
       "      <td>hai fifa kami indonesia ada tim yang paling ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@yellowsea_sun @idextratime Kenapa bisa lolos ...</td>\n",
       "      <td>kenapa bisa lolos jadi wasit ya chicken stupid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  @FIFAWorldCup Hai FIFA ....kami Indonesia dan ...   \n",
       "1  @emhendraka Nek ra itungan ngko koyo wasit sok...   \n",
       "2  @SiaranBolaLive Tak kasi sambel matanya ini wa...   \n",
       "3  @fifamedia Hai FIFA ....kami Indonesia ....kam...   \n",
       "4  @yellowsea_sun @idextratime Kenapa bisa lolos ...   \n",
       "\n",
       "                                         Tweet_clean  \n",
       "0  hai fifa kami indonesia dan ada tim yang palin...  \n",
       "1         nek ra itungan ngko koyo wasit soko om kae  \n",
       "2  tak kasi sambel mata ini wasit dajal biar seka...  \n",
       "3  hai fifa kami indonesia ada tim yang paling ke...  \n",
       "4  kenapa bisa lolos jadi wasit ya chicken stupid...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the cleansing and stemming to the 'Tweet' column\n",
    "data['Tweet_clean'] = data['Tweet'].apply(cleansing_with_stemming)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment based on cleaned text\n",
    "def predict_sentiment(sent):\n",
    "    # Feature extraction\n",
    "    text_feature = feature_bow.transform([sent])\n",
    "    # Predict using the Naive Bayes model\n",
    "    return model_nb.predict(text_feature)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet  \\\n",
      "0  @FIFAWorldCup Hai FIFA ....kami Indonesia dan ...   \n",
      "1  @emhendraka Nek ra itungan ngko koyo wasit sok...   \n",
      "2  @SiaranBolaLive Tak kasi sambel matanya ini wa...   \n",
      "3  @fifamedia Hai FIFA ....kami Indonesia ....kam...   \n",
      "4  @yellowsea_sun @idextratime Kenapa bisa lolos ...   \n",
      "\n",
      "                                         Tweet_clean predicted_sentiment  \n",
      "0  hai fifa kami indonesia dan ada tim yang palin...            negative  \n",
      "1         nek ra itungan ngko koyo wasit soko om kae            negative  \n",
      "2  tak kasi sambel mata ini wasit dajal biar seka...            negative  \n",
      "3  hai fifa kami indonesia ada tim yang paling ke...            negative  \n",
      "4  kenapa bisa lolos jadi wasit ya chicken stupid...             neutral  \n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for the cleaned tweets\n",
    "data['predicted_sentiment'] = data['Tweet_clean'].apply(predict_sentiment)\n",
    "\n",
    "# Display the DataFrame with predicted sentiments\n",
    "print(data[['Tweet', 'Tweet_clean', 'predicted_sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_clean</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@FIFAWorldCup Hai FIFA ....kami Indonesia dan ...</td>\n",
       "      <td>hai fifa kami indonesia dan ada tim yang palin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@emhendraka Nek ra itungan ngko koyo wasit sok...</td>\n",
       "      <td>nek ra itungan ngko koyo wasit soko om kae</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SiaranBolaLive Tak kasi sambel matanya ini wa...</td>\n",
       "      <td>tak kasi sambel mata ini wasit dajal biar seka...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@fifamedia Hai FIFA ....kami Indonesia ....kam...</td>\n",
       "      <td>hai fifa kami indonesia ada tim yang paling ke...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@yellowsea_sun @idextratime Kenapa bisa lolos ...</td>\n",
       "      <td>kenapa bisa lolos jadi wasit ya chicken stupid...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  @FIFAWorldCup Hai FIFA ....kami Indonesia dan ...   \n",
       "1  @emhendraka Nek ra itungan ngko koyo wasit sok...   \n",
       "2  @SiaranBolaLive Tak kasi sambel matanya ini wa...   \n",
       "3  @fifamedia Hai FIFA ....kami Indonesia ....kam...   \n",
       "4  @yellowsea_sun @idextratime Kenapa bisa lolos ...   \n",
       "\n",
       "                                         Tweet_clean predicted_sentiment  \n",
       "0  hai fifa kami indonesia dan ada tim yang palin...            negative  \n",
       "1         nek ra itungan ngko koyo wasit soko om kae            negative  \n",
       "2  tak kasi sambel mata ini wasit dajal biar seka...            negative  \n",
       "3  hai fifa kami indonesia ada tim yang paling ke...            negative  \n",
       "4  kenapa bisa lolos jadi wasit ya chicken stupid...             neutral  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('data/dataset_predicted_sentiment.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
